{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                                           Harris Corner Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from scipy.ndimage import convolve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaus(size, sigma):\n",
    "    center = size // 2\n",
    "    gaussian = np.zeros((size, size))\n",
    "\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            x = i - center\n",
    "            y = j - center\n",
    "            gaussian[i, j] = np.exp(-((x ** 2 + y ** 2) / (2 * sigma ** 2)))\n",
    "\n",
    "    # Normalize the Gaussian kernel\n",
    "    gaussian /= np.sum(gaussian)\n",
    "    return gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients\n",
    "Certainly! The Sobel operator uses convolution kernels to approximate the image gradients in the x and y directions. These kernels are typically 3x3 matrices.\n",
    "\n",
    "### 1. **Sobel Kernel for X-Direction (\\( I_x \\))**:\n",
    "This kernel is used to compute the gradient in the horizontal (x) direction:\n",
    "\n",
    "$$\n",
    "G_x = \\begin{pmatrix}\n",
    "-1 & 0 & +1 \\\\\n",
    "-2 & 0 & +2 \\\\\n",
    "-1 & 0 & +1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### 2. **Sobel Kernel for Y-Direction (\\( I_y \\))**:\n",
    "This kernel is used to compute the gradient in the vertical (y) direction:\n",
    "\n",
    "$$\n",
    "G_y = \\begin{pmatrix}\n",
    "-1 & -2 & -1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "+1 & +2 & +1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Applying the Sobel Kernels:\n",
    "To compute the image gradients, the Sobel kernels are convolved with the image:\n",
    "\n",
    "$$\n",
    "I_x(x, y) = G_x * I(x, y)\n",
    "$$\n",
    "$$\n",
    "I_y(x, y) = G_y * I(x, y)\n",
    "$$\n",
    "\n",
    "Here, \\( * \\) denotes the convolution operation, and \\( I(x, y) \\) is the intensity value at pixel \\( (x, y) \\). The result of the convolution will give you the gradients in the x and y directions, which are essential for constructing the structure tensor in the Harris Corner Detection algorithm.\n",
    "\n",
    "These gradients highlight edges and transitions in the image, with \\( I_x \\) capturing horizontal changes and \\( I_y \\) capturing vertical changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(image):\n",
    "     I_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)  # Gradient in X direction\n",
    "     I_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)  # Gradient in Y direction\n",
    "\n",
    "     return I_x, I_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure Tensor\n",
    "\n",
    "The smoothed gradients are then used to form the structure tensor \\( M(x, y) \\) at each pixel \\( (x, y) \\):\n",
    "\n",
    "$$\n",
    "M(x, y) = \\begin{pmatrix}\n",
    "I_x^2 & I_x \\cdot I_y \\\\\n",
    "I_x \\cdot I_y  & I_y^2 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_structure_tensor(I_x_smooth, I_y_smooth):\n",
    "    I_x2 = I_x_smooth ** 2\n",
    "    I_y2 = I_y_smooth ** 2\n",
    "    I_xy = I_x_smooth * I_y_smooth\n",
    "    return I_x2, I_y2, I_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Window\n",
    "\n",
    "In practice, these two steps (applying the Gaussian filter and forming the structure tensor) are often combined. The structure tensor is directly constructed using the smoothed gradient products:\n",
    "\n",
    "$$\n",
    "M(x, y) = \\begin{pmatrix}\n",
    "I_x^2 * w(x, y) & I_x \\cdot I_y * w(x, y) \\\\\n",
    "I_x \\cdot I_y * w(x, y) & I_y^2 * w(x, y)\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_window(I_x2, I_y2, I_xy, size, sigma):\n",
    "    gaussian_kernel = gaus(size, sigma)  # Create custom Gaussian kernel\n",
    "    w_x2 = convolve(I_x2, gaussian_kernel)\n",
    "    w_y2 = convolve(I_y2, gaussian_kernel)\n",
    "    w_xy = convolve(I_xy, gaussian_kernel)\n",
    "    return w_x2, w_y2, w_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corner Response\n",
    "\n",
    "   First, compute the corner response function \\( R(x, y) \\) for all pixels in the image:\n",
    "\n",
    "   $$\n",
    "   R(x, y) = \\text{det}(M(x, y)) - k \\cdot (\\text{trace}(M(x, y)))^2\n",
    "   $$\n",
    "\n",
    "   where \\( M(x, y) \\) is the structure tensor at pixel \\( (x, y) \\).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corner_response(w_x2, w_y2, w_xy, k):\n",
    "    det_M = w_x2 * w_y2 - w_xy**2\n",
    "    trace_M = w_x2 + w_y2\n",
    "    R = det_M - k * (trace_M**2)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding\n",
    " \n",
    "   Apply a threshold \\( T \\) to filter out weak responses:\n",
    "\n",
    "   $$\n",
    "   R(x, y) \\leftarrow \\begin{cases} \n",
    "   R(x, y) & \\text{if } R(x, y) > T \\\\\n",
    "   0 & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(R, threshold_ratio):\n",
    "    R_max = np.max(R)\n",
    "    threshold = threshold_ratio * R_max\n",
    "    R[R < threshold] = 0\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non maxima supression\n",
    "\n",
    "For each pixel \\( (x_0, y_0) \\) with a non-zero response after thresholding, check if it is a local maximum within a specified neighborhood \\( \\mathcal{N} \\) (e.g., a 3x3 or 5x5 window):\n",
    "\n",
    "   $$\n",
    "   R(x_0, y_0) \\text{ is a local maximum if } R(x_0, y_0) \\geq R(x, y) \\text{ for all } (x, y) \\in \\mathcal{N}\n",
    "   $$\n",
    "\n",
    "   Specifically:\n",
    "   - If \\( R(x_0, y_0) \\) is greater than or equal to all other \\( R(x, y) \\) values in the neighborhood \\( \\mathcal{N} \\), it is considered a local maximum and retained as a corner.\n",
    "   - If \\( R(x_0, y_0) \\) is not the maximum in its neighborhood, it is suppressed (set to zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(R, window_size=7):\n",
    "    \n",
    "    suppressed_R = np.zeros_like(R)\n",
    "    \n",
    "    for i in range(window_size, R.shape[0] - window_size):\n",
    "        for j in range(window_size, R.shape[1] - window_size):\n",
    "            window = R[i-window_size:i+window_size+1, j-window_size:j+window_size+1]\n",
    "            if R[i, j] == np.max(window):\n",
    "                suppressed_R[i, j] = R[i, j]\n",
    "    \n",
    "    return suppressed_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corner Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_corners(image, corners, output_path):\n",
    "\n",
    "    # Get the original image dimensions\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Calculate figsize in inches based on the image size and DPI\n",
    "    dpi = 100  # Dots per inch\n",
    "    figsize = (width / dpi, height / dpi)\n",
    "    \n",
    "    # Create a figure with the size of the original image\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.imshow(image, cmap='gray' if len(image.shape) == 2 else None)  # Use grayscale colormap if image is grayscale\n",
    "    \n",
    "    # Overlay red plus signs on the corner locations\n",
    "    y_coords, x_coords = corners[:, 0], corners[:, 1]\n",
    "    plt.scatter(x_coords, y_coords, c='red', s=50, marker='+')  # 's' is the size of the marker\n",
    "    \n",
    "    # Customize and display the plot\n",
    "    plt.axis('off')  # Hide axis\n",
    "    # plt.title('Image with Detected Corners')  # Title line is commented out\n",
    "    \n",
    "    # Save the image with the same dimensions as the original\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0, dpi=1.5*dpi)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harris Corner Responce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harris_corner_detection(image, k=0.06, threshold_ratio=0.01, size=5, sigma=1):\n",
    "    # Step 1: Load the original image and convert to grayscale\n",
    "    # The original image is loaded and converted to grayscale to simplify the corner detection process.\n",
    "    image_org = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Create a custom Gaussian kernel using the specified size and sigma values\n",
    "    # The Gaussian kernel is used to smooth the image, reducing noise before gradient computation.\n",
    "    gaussian_kernel = gaus(size, sigma)\n",
    "    \n",
    "    # Apply Gaussian smoothing to the original image\n",
    "    # This step smooths the entire image before gradient computation.\n",
    "    image = convolve(image_org, gaussian_kernel)\n",
    "    \n",
    "    # Step 2: Compute gradients in the x and y directions using Sobel operator\n",
    "    # This calculates the first derivatives of the image, essential for computing the structure tensor.\n",
    "    I_x, I_y = compute_gradients(image)\n",
    "    \n",
    "    # Step 3: Compute the structure tensor elements\n",
    "    # I_x2 = I_x^2, I_y2 = I_y^2, I_xy = I_x * I_y\n",
    "    # These elements represent the gradient information in a matrix form.\n",
    "    I_x2, I_y2, I_xy = compute_structure_tensor(I_x, I_y)\n",
    "    \n",
    "    # Apply Gaussian windowing to the structure tensor elements\n",
    "    # This step uses a Gaussian window to smooth the elements of the structure tensor.\n",
    "    w_x2, w_y2, w_xy = apply_window(I_x2, I_y2, I_xy, size, sigma)\n",
    "    \n",
    "    # Step 4: Compute the Harris corner response R\n",
    "    # The corner response is computed using the determinant and trace of the structure tensor matrix.\n",
    "    R = compute_corner_response(w_x2, w_y2, w_xy, k=k)\n",
    "    \n",
    "    # Step 5: Apply a threshold to the corner response\n",
    "    # This removes weak corner responses by setting them to zero.\n",
    "    R= apply_threshold(R, threshold_ratio=threshold_ratio)\n",
    "    \n",
    "    # Step 6: Apply non-maximum suppression to the thresholded response\n",
    "    # This step keeps only the local maxima, which represent the strongest corners.\n",
    "    R_sup = non_max_suppression(R)\n",
    "    \n",
    "    # Step 7: Extract corner points from the suppressed response\n",
    "    # Corners are identified as non-zero points in the suppressed response matrix.\n",
    "    corners = np.argwhere(R_sup> 0)\n",
    "    \n",
    "    return image_org,R,corners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization process centers the data around zero by subtracting the mean and then scales it by the standard deviation, ensuring that the patch has a standard deviation of 1. A small constant \\( \\epsilon \\) is added to the denominator to avoid division by zero.\n",
    "\n",
    "### Mathematical Form\n",
    "\n",
    "Given a patch \\( X \\) (which is typically a small matrix of pixel values), the normalized patch \\( \\tilde{X} \\) is computed as:\n",
    "\n",
    "$$\n",
    "\\tilde{X} = \\frac{X - \\mu_X}{\\sigma_X + \\epsilon}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ \\mu_X = \\frac{1}{N} \\sum_{i=1}^{N} X_i $ is the mean of the patch $ X 4.\n",
    "- $ \\sigma_X = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\mu_X)^2} $ is the standard deviation of the patch $ X $.\n",
    "- $ \\epsilon $ is a small constant added to the standard deviation to prevent division by zero.\n",
    "- $ N $ is the total number of pixels in the patch.\n",
    "\n",
    "\n",
    "\n",
    "### Final Formula:\n",
    "$$\n",
    "\\tilde{X} = \\frac{X - \\mu_X}{\\sigma_X + \\epsilon}\n",
    "$$\n",
    "\n",
    "This process ensures that the patch \\( X \\) is normalized, which is often a critical step in many image processing tasks, such as feature matching or machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_normalized_patches(image, keypoints, patch_size=32,epsilon=1e-7):\n",
    "    patches = []\n",
    "\n",
    "    # Pad the image to handle keypoints near borders\n",
    "    border_size = patch_size // 2\n",
    "    padded_image = cv2.copyMakeBorder(image, border_size, border_size, border_size, border_size, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    for kp in keypoints:\n",
    "        # Convert keypoint coordinates to integers\n",
    "        x, y = int(kp[1]), int(kp[0])  # Note: keypoints are (row, col) and image is (x, y)\n",
    "\n",
    "        # Extract patch from padded image\n",
    "        patch = padded_image[y:y+patch_size, x:x+patch_size].astype(np.float32)\n",
    "\n",
    "        mean = np.mean(patch)\n",
    "        std = np.std(patch)\n",
    "        normalized_patch = (patch - mean) / (std + epsilon)\n",
    "        patches.append(normalized_patch)\n",
    "\n",
    "    return patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_patches(image, keypoints, output_path, patch_size=32):\n",
    " \n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Border size (half of the patch size)\n",
    "    border_size = patch_size // 2\n",
    "\n",
    "    for kp in keypoints:\n",
    "        x, y = int(kp[1]), int(kp[0])\n",
    "\n",
    "        # Draw a patch centered on the keypoint\n",
    "        top_left = (x - border_size, y - border_size)\n",
    "        rect = patches.Rectangle(top_left, patch_size, patch_size,\n",
    "                                 linewidth=2, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance between patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(patches1, patches2):\n",
    "    n = len(patches1)\n",
    "    m = len(patches2)\n",
    "    D = np.zeros((n, m))\n",
    "    max_shape = max(patches1[i].flatten().shape[0] for i in range(n))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            patch1 = patches1[i].flatten()\n",
    "            patch2 = patches2[j].flatten()\n",
    "            patch1_shape = patch1.shape[0]\n",
    "            patch2_shape = patch2.shape[0]\n",
    "            if patch1_shape == patch2_shape:\n",
    "                D[i, j] = np.sum((patch1 - patch2) ** 2)\n",
    "            else:\n",
    "                if patch1_shape < max_shape:\n",
    "                    patch1 = np.pad(patch1, (0, max_shape - patch1_shape), 'constant')\n",
    "                if patch2_shape < max_shape:\n",
    "                    patch2 = np.pad(patch2, (0, max_shape - patch2_shape), 'constant')\n",
    "                D[i, j] = np.sum((patch2 - patch1) ** 2)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(D):\n",
    "    M1, M2 = D.shape\n",
    "    matches = []\n",
    "\n",
    "    for j in range(M2):\n",
    "        i = np.argmin(D[:, j])  # Find the index i that minimizes the distance for keypoint j in frame k+1\n",
    "        matches.append((i, j))  # Append the matching pair (i, j) to the list\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches_robust(D, ratio_threshold=0.7):\n",
    "    n, m = D.shape\n",
    "    matches = []\n",
    "\n",
    "    # Step 1: 1NN/2NN Ratio Test\n",
    "    for i in range(n):\n",
    "        distances = D[i, :]\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        best_match = sorted_indices[0]\n",
    "        second_best_match = sorted_indices[1]\n",
    "\n",
    "        # Calculate the ratio\n",
    "        ratio = distances[best_match] / distances[second_best_match]\n",
    "\n",
    "        # Accept the match if it passes the ratio test\n",
    "        if ratio < ratio_threshold:\n",
    "            matches.append((i, best_match))\n",
    "\n",
    "    # Step 2: Cross-Validation Check\n",
    "    robust_matches = []\n",
    "    for (i, j) in matches:\n",
    "        reverse_match = np.argmin(D[:, j])\n",
    "        if reverse_match == i:\n",
    "            robust_matches.append((i, j))\n",
    "\n",
    "    return robust_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches_vis(image_tgt, list_keypoints_src, list_keypoints_tgt, matches, output_path, dpi=100):\n",
    " \n",
    "    # Convert grayscale image to RGB\n",
    "    if len(image_tgt.shape) == 2:  # If the image is grayscale\n",
    "        image_color = cv2.merge([image_tgt, image_tgt, image_tgt]).astype(np.uint8)\n",
    "    else:\n",
    "        image_color = image_tgt.copy()\n",
    "\n",
    "    # Draw cross markers for target keypoints in red\n",
    "    for keyp in list_keypoints_tgt:\n",
    "        cv2.drawMarker(image_color, (int(keyp[1]), int(keyp[0])), (0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=10, thickness=2, line_type=cv2.LINE_8)\n",
    "\n",
    "    # Draw lines between matched keypoints in green\n",
    "    for (i, j) in matches:\n",
    "        coord_src, coord_tgt = list_keypoints_src[i, :], list_keypoints_tgt[j, :]\n",
    "        cv2.line(image_color, (int(coord_tgt[1]), int(coord_tgt[0])), (int(coord_src[1]), int(coord_src[0])), (0, 255, 0), 1)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    height, width = image_tgt.shape[:2]\n",
    "\n",
    "    # Display image with the same size as the original image\n",
    "    plt.figure(figsize=(width / dpi, height / dpi), dpi=dpi)  # Set figure size\n",
    "    plt.imshow(cv2.cvtColor(image_color, cv2.COLOR_BGR2RGB))  # Convert to RGB for displaying\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0, dpi=1.5*dpi)\n",
    "    plt.close()\n",
    "\n",
    "    return image_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_in_folder(folder_path, image_extensions=('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
    "    count = 0\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(image_extensions):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(input_folder, output_folder):\n",
    "    # Create the main output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Define subfolders for each type of output\n",
    "    harris_folder = os.path.join(output_folder, 'harris')\n",
    "    patches_folder = os.path.join(output_folder, 'patches')\n",
    "    matches_folder = os.path.join(output_folder, 'matches')\n",
    "    robust_matches_folder = os.path.join(output_folder, 'robust_matches')\n",
    "\n",
    "    # Create subfolders if they don't exist\n",
    "    for folder in [harris_folder, patches_folder, matches_folder, robust_matches_folder]:\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "    # List all image files in the input folder\n",
    "    image_files = sorted([f for f in os.listdir(input_folder) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'))])\n",
    "\n",
    "    # Loop through all the images\n",
    "    for i in range(len(image_files) - 1):\n",
    "        image1_path = os.path.join(input_folder, image_files[i])\n",
    "        image2_path = os.path.join(input_folder, image_files[i+1])\n",
    "\n",
    "        image1 = cv2.imread(image1_path)\n",
    "        image2 = cv2.imread(image2_path)\n",
    "\n",
    "        # Apply Harris corner detection\n",
    "        image, R, keypoints1 = harris_corner_detection(image1)\n",
    "        image, R, keypoints2 = harris_corner_detection(image2)\n",
    "\n",
    "        patches1 = extract_normalized_patches(image1, keypoints1)\n",
    "        patches2 = extract_normalized_patches(image2, keypoints2)\n",
    "\n",
    "        D = distance_matrix(patches1, patches2)\n",
    "        matches = find_matches(D)\n",
    "        matche = find_matches_robust(D)\n",
    "\n",
    "        # Generate base filenames with zero-padded numbers\n",
    "        base_filename1 = os.path.splitext(image_files[i])[0].zfill(6)\n",
    "        base_filename2 = os.path.splitext(image_files[i+1])[0].zfill(6)\n",
    "\n",
    "        # Save Harris corners\n",
    "        harris_output1 = os.path.join(harris_folder, f'{base_filename1}harris.jpg')\n",
    "        harris_output2 = os.path.join(harris_folder, f'{base_filename2}harris.jpg')\n",
    "        plot_image_with_corners(image1, keypoints1, harris_output1)\n",
    "        plot_image_with_corners(image2, keypoints2, harris_output2)\n",
    "\n",
    "        # Save patches\n",
    "        patches_output1 = os.path.join(patches_folder, f'{base_filename1}patches.jpg')\n",
    "        patches_output2 = os.path.join(patches_folder, f'{base_filename2}patches.jpg')\n",
    "        plot_patches(image1, keypoints1, patches_output1)\n",
    "        plot_patches(image2, keypoints2, patches_output2)\n",
    "\n",
    "        # Save matches\n",
    "        matches_output = os.path.join(matches_folder, f'{base_filename1}matching.jpg')\n",
    "        matches_vis(image2, keypoints1, keypoints2, matches, matches_output)\n",
    "\n",
    "        # Save robust matches\n",
    "        robust_matches_output = os.path.join(robust_matches_folder, f'{base_filename1}matching_robust.jpg')\n",
    "        matches_vis(image2, keypoints1, keypoints2, matche, robust_matches_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_from_images(image_folder, output_video_path, fps=30):\n",
    "    # Get a list of all image files in the folder (sorted by filename)\n",
    "    image_files = [f for f in sorted(os.listdir(image_folder)) if f.endswith(('png', 'jpg', 'jpeg', 'bmp'))]\n",
    "    \n",
    "    # Read the first image to get the dimensions\n",
    "    first_image_path = os.path.join(image_folder, image_files[0])\n",
    "    first_image = cv2.imread(first_image_path)\n",
    "    height, width, layers = first_image.shape\n",
    "    \n",
    "    # Initialize the VideoWriter\n",
    "    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    \n",
    "    # Add each image as a frame to the video\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        video_writer.write(image)\n",
    "    \n",
    "    # Release the VideoWriter\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved at {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'U:\\\\Final Year Project\\\\FYP-2\\\\Tasks\\\\Feature Extraction\\\\CV_Assignment_1_CornerTracker\\\\images'\n",
    "\n",
    "output_folder = 'U:\\\\Final Year Project\\\\FYP-2\\\\Tasks\\\\Feature Extraction\\\\CV_Assignment_1_CornerTracker'\n",
    "\n",
    "output_video_file = 'U:\\\\Final Year Project\\\\FYP-2\\\\Tasks\\\\Feature Extraction\\\\CV_Assignment_1_CornerTracker\\\\videos'\n",
    "\n",
    "harris_folder = os.path.join(output_folder, 'harris')\n",
    "patches_folder = os.path.join(output_folder, 'patches')\n",
    "matches_folder = os.path.join(output_folder, 'matches')\n",
    "robust_matches_folder = os.path.join(output_folder, 'robust_matches')\n",
    "\n",
    "# Define the paths for the output videos\n",
    "output_video_harris = os.path.join(output_video_file, 'harris_video.mp4')\n",
    "output_video_patches = os.path.join(output_video_file, 'patches_video.mp4')\n",
    "output_video_matches = os.path.join(output_video_file, 'matches_video.mp4')\n",
    "output_video_robust_matches = os.path.join(output_video_file, 'robust_matches_video.mp4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of images in the folder:\n",
      " 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count=count_images_in_folder(input_folder)\n",
    "\n",
    "print(\"No of images in the folder:\\n\",count)\n",
    "\n",
    "process_images(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved at U:\\Final Year Project\\FYP-2\\Tasks\\Feature Extraction\\CV_Assignment_1_CornerTracker\\videos\\harris_video.mp4\n",
      "Video saved at U:\\Final Year Project\\FYP-2\\Tasks\\Feature Extraction\\CV_Assignment_1_CornerTracker\\videos\\patches_video.mp4\n",
      "Video saved at U:\\Final Year Project\\FYP-2\\Tasks\\Feature Extraction\\CV_Assignment_1_CornerTracker\\videos\\matches_video.mp4\n",
      "Video saved at U:\\Final Year Project\\FYP-2\\Tasks\\Feature Extraction\\CV_Assignment_1_CornerTracker\\videos\\robust_matches_video.mp4\n"
     ]
    }
   ],
   "source": [
    "# Call the create_video_from_images function for each subfolder\n",
    "\n",
    "# 1. Create video from Harris corners images\n",
    "create_video_from_images(harris_folder, output_video_harris)\n",
    "\n",
    "# 2. Create video from patches images\n",
    "create_video_from_images(patches_folder, output_video_patches)\n",
    "\n",
    "# 3. Create video from matches images\n",
    "create_video_from_images(matches_folder, output_video_matches)\n",
    "\n",
    "# 4. Create video from robust matches images\n",
    "create_video_from_images(robust_matches_folder, output_video_robust_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_image_path = 'U:\\\\Final Year Project\\\\FYP-2\\\\Tasks\\\\Feature Extraction\\\\CV_Assignment_1_CornerTracker\\\\images\\\\000000.png'\n",
    "# output_image_path = 'U:\\\\Final Year Project\\\\FYP-2\\\\Tasks\\\\Feature Extraction\\\\CV_Assignment_1_CornerTracker\\\\out_images\\\\000000_hc.png'\n",
    "\n",
    "# image = cv2.imread(input_image_path)\n",
    "\n",
    "# # Convert the image to grayscale\n",
    "# gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# gray_image = np.float32(gray_image)\n",
    "\n",
    "# # Perform Harris corner detection\n",
    "# corners = cv2.cornerHarris(gray_image, blockSize=2, ksize=3, k=0.06)\n",
    "\n",
    "# # Dilate the result to enhance corner points\n",
    "# corners = cv2.dilate(corners, None)\n",
    "\n",
    "# # Mark corners in red on the original image\n",
    "# image[corners > 0.01 * corners.max()] = [0, 0, 255]\n",
    "\n",
    "# # Convert BGR to RGB for correct color display in Matplotlib\n",
    "# rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# # Plot the image with detected corners\n",
    "# plt.figure(figsize=(10, 10))  # Adjust size as needed\n",
    "# plt.imshow(rgb_image)\n",
    "# plt.axis('off')  # Hide axis\n",
    "# plt.title('Image with Detected Corners')\n",
    "\n",
    "# # Save and show the image\n",
    "# plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0)\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
